{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6ce19-52da-43d3-9ad1-356b25fc2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('popular')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d70d342-5f3a-4c20-9b55-e398cfa356ee",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "- Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b995d7f7-3120-4379-aabd-892bfee57d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokenizer: ['Hello world!', 'Welcome to this amazing party.']\n",
      "Sentence tokenizer: ['Hello', 'world', '!', 'Welcome', 'to', 'this', 'amazing', 'party', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "example=\"Hello world! Welcome to this amazing party.\"\n",
    "print(f\"Sentence tokenizer: {sent_tokenize(example)}\")\n",
    "print(f\"Sentence tokenizer: {word_tokenize(example)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a79bf-7e5c-4ee0-bc7a-58cb38cbfbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13270d24-9991-428f-a40b-6af35016959c",
   "metadata": {},
   "source": [
    "## Stop Words Removal\n",
    "- Stop words are words with very little meaning such as a,an,the,etc.\n",
    "- Stop words are filtered out before processing  natural language as they don't reveal much information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "027375ef-fef6-430d-b072-59f6594bc827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words examples : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "print(\"Stop words examples :\",stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ee3646-df4b-442e-86a7-3a8d55123b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens          : ['An', 'apple', 'a', 'day', 'keeps', 'a', 'doctor', 'away', '.']\n",
      "filtered tokens : ['An', 'apple', 'day', 'keeps', 'doctor', 'away', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example=\"An apple a day keeps a doctor away.\"\n",
    "tokens=word_tokenize(example)\n",
    "\n",
    "#removing stop words from tokens\n",
    "filtered_sentence=[tok for tok in tokens if tok not in stop_words]\n",
    "\n",
    "print(\"tokens          :\",tokens)\n",
    "print(\"filtered tokens :\",filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61866c-147f-4801-8c9f-c0f7d893e967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fd55b47-9d84-40b5-a05b-1b47505ba2f7",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "- Stemming involves reducing a word to a base(root) form by removing suffixes \n",
    "- There are various stemmng algo: Porter Stemmer, Lancaster stemmer,Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ae8645f-db58-48f1-aea4-3e5651093c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Befor stemming tokens: Stemming involves reducing a word to a base(root) form by removing suffixes\n",
      "After stemming tokens: stem involv reduc a word to a base ( root ) form by remov suffix "
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps=PorterStemmer()\n",
    "example=\"Stemming involves reducing a word to a base(root) form by removing suffixes\"\n",
    "tokens=word_tokenize(example)\n",
    "print(\"Befor stemming tokens:\",example)\n",
    "print(\"After stemming tokens: \",end='')\n",
    "for word in tokens:\n",
    "    print(ps.stem(word),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d3ff9-90d7-4501-bf74-7decdca4885f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cd79bed-809f-46e7-a32e-2b01c2139be7",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "Lemmatization is a text normalization technique used in Natural Language Processing (NLP), that switches any kind of a word to its base root mode. Lemmatization is responsible for grouping different inflected forms of words into the root form, having the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbddd4cc-c4e2-42e7-8260-8ba4f4f689f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Befor lemmatizing : ['cacti', 'geese']\n",
      "After lemmatizing : cactus, goose, "
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "example=[\"cacti\",\"geese\"]\n",
    "print(\"Befor lemmatizing :\",example)\n",
    "print(\"After lemmatizing : \",end='')\n",
    "for ex in example:\n",
    "    print(lemmatizer.lemmatize(ex),end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32ddc2-57f9-464d-af74-d58a595ed981",
   "metadata": {},
   "source": [
    "### Stemming v/s Lemmatization\n",
    "- Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling.\tLemmatization considers the context and converts the word to its meaningful base form, which is called Lemma.\n",
    "- For instance, stemming the word ‘reducing‘ would return ‘reduc‘ and lemmatizing the word ‘geese‘ would return ‘goose‘.\n",
    "- Stemming is used in case of large dataset where performance is an issue.\t\n",
    "- Lemmatization is computationally expensive since it involves look-up tables and what not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95b0bf0-cd9b-4607-9df9-cdb5456eadbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "782c1f78-55fd-407c-b675-1ebdb404d98f",
   "metadata": {},
   "source": [
    "## POS Taggging\n",
    "- Parts Of Speech Tagging is used to tag words with their corresponding parts of speech tag based on its context and definition.\n",
    "- Pos tags are useful for lemmatization,named entity recognition (NER), and extracting relationshib between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e931d1-2bd5-4938-a536-0ea0b8ca6df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Alice', 'NOUN'), ('wrote', 'VERB'), ('program', 'NOUN')]\n",
      "[('Alice', 'NNP'), ('wrote', 'VBD'), ('program', 'NN')] you can get  full list of abbrevations on websites\n"
     ]
    }
   ],
   "source": [
    "example=\"Alice wrote a program\"\n",
    "#tokenizing\n",
    "tokens=word_tokenize(example)\n",
    "#removing stop words\n",
    "filtered_tokens=[token for token in tokens if token not in stop_words]\n",
    "#pos tagging\n",
    "print(nltk.pos_tag(filtered_tokens ,tagset='universal'))\n",
    "print(nltk.pos_tag(filtered_tokens ),\"you can get  full list of abbrevations on websites\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f0bfc-98f2-440d-aff8-c3287a43dd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5901b47-7ad8-4329-b716-c2c43485725b",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER)\n",
    "- NER extracts real world entity from text and sorts it into predefined caegories like name, location, organization,etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fa1dc2d-fb15-434a-9785-15e3f8cccd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example        : Satya is CEO of Microsoft\n",
      "named_entities : [('Satya', 'PERSON'), ('CEO Microsoft', 'ORGANIZATION')]\n"
     ]
    }
   ],
   "source": [
    "example=\"Satya is CEO of Microsoft\"\n",
    "#tokenizing\n",
    "tokens=word_tokenize(example)\n",
    "#removing stop words\n",
    "filtered_tokens=[token for token in tokens if token not in stop_words]\n",
    "#pos tagging\n",
    "tagged_sentences=nltk.pos_tag(filtered_tokens)\n",
    "#Use NLTK's currently recommended named entity chunker to chunk the given list of tagged tokens.\n",
    "ne_chunk_sentences=nltk.ne_chunk(tagged_sentences)\n",
    "\n",
    "\n",
    "named_entities=[]\n",
    "for tagged_tree in ne_chunk_sentences:\n",
    "    if hasattr(tagged_tree,'label'):\n",
    "        entity_name=' '.join( [name[0] for name in tagged_tree.leaves()] )\n",
    "        entity_type=tagged_tree.label()\n",
    "        named_entities.append((entity_name,entity_type))\n",
    "        \n",
    "print('example        :',example)       \n",
    "print('named_entities :',named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9792328-db9b-4b22-bfed-22eb7aea4703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbc91a0e-08d3-4c14-a1c2-ac9204720e56",
   "metadata": {},
   "source": [
    "## NLP Process Workflow\n",
    "\n",
    "- Tokenization - Stop word removal - Stemming and Lemmatization - POS Tagging - Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "448bcbc0-eedf-4b3b-ac4b-6ecc30246116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "085891fd-aa63-43de-87c7-d9c06eb99d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('brown_corpus_ca10.txt','r') as myfile:\n",
    "    data=myfile.read().lower().replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f499d6b2-d68d-489b-99b0-979f14d5a789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\tvincent/np g./np ierulli/np has/hvz been/ben appointed/vbn temporary/jj assistant/nn district/nn attorney/nn ,/, it/pps was/bedz announced/vbn monday/nr by/in charles/np e./np raymond/np ,/, district/nn-tl attorney/nn-tl ./.   \\tierulli/np will/md replace/vb desmond/np d./np connall/np who/wps has/hvz been/ben called/vbn to/in active/jj military/jj service/nn but/cc is/bez expected/vbn back/rb on/in the/at job/nn by/in march/np 31/cd ./.   \\tierulli/np ,/, 29/cd ,/, has/hvz been/ben practicing/'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c598ae52-1379-4e34-a2e2-1746a18ff471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we can see many words have ending in format /something which dosent make sense, so remove it with regex.\n",
    "#Example: has/hvz, attorney/nn-tl, etc.\n",
    "import re\n",
    "data=re.sub('/[a-z0-9]*\\D?\\D?[a-z0-9]* ',' ',data)   #This replaces given pattern with space in 'data' text\n",
    "                                                    #\\D Matches any non-digits for cases like '-' or '$'\n",
    "                                                    # '?' matches 0 or 1 occurence\n",
    "\n",
    "data=re.sub('\\s',' ',data)      # '\\s' Matches whitespace characters, which include the \\t, \\n, \\r, and space characters.\n",
    "data=re.sub(\"''\",'',data)  \n",
    "data=re.sub(\"``\",'',data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8a432c5-7104-4819-b1ea-aff5d14f25d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   vincent g. ierulli has been appointed temporary assistant district attorney , it was announced monday by charles e. raymond , district attorney .   ierulli will replace desmond d. connall who has been called to active military service but is expected back on the job by march 31 .   ierulli , 29 , has been practicing in portland since november , 1959 . he is a graduate of portland university and the northwestern college of law . he is married and the father of three children .   helping foreig'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e17696e1-c62a-439f-8471-f13ee3dd679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize,word_tokenize\n",
    "word_tokens=word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92ff44f8-2cf7-4a94-aaf2-cc14d5cb4fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vincent', 'g.', 'ierulli', 'appointed', 'temporary', 'assistant', 'district', 'attorney', 'announced', 'monday']\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords and punchuations\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation   #punctuation='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "\n",
    "stopwords_en=stopwords.words('english')\n",
    "stopwords_with_punctuations=set(stopwords_en).union(set(punctuation)) #combining stopwords and punctuations\n",
    "\n",
    "filtered_tokens=[token for token in word_tokens if token not in stopwords_with_punctuations]\n",
    "\n",
    "print(filtered_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cedf4ae4-45ba-439e-a424-b39f3f50efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer    : ['vincent', 'g.', 'ierulli', 'appoint', 'temporari', 'assist', 'district', 'attorney', 'announc', 'monday']\n",
      "Lemmatizer : ['vincent', 'g.', 'ierulli', 'appointed', 'temporary', 'assistant', 'district', 'attorney', 'announced', 'monday']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
    "ps=PorterStemmer()\n",
    "wnl=WordNetLemmatizer()\n",
    "\n",
    "stem_tokens=[ps.stem(token) for token in filtered_tokens ]\n",
    "wnl_tokens=[wnl.lemmatize(token) for token in filtered_tokens ]   \n",
    "\n",
    "print(\"Stemmer    :\",stem_tokens[:10])\n",
    "print(\"Lemmatizer :\",wnl_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24340d41-b994-40ac-aa47-e57414934ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vincent', 'NN'), ('g.', 'NN'), ('ierulli', 'NN'), ('appointed', 'VBD'), ('temporary', 'JJ'), ('assistant', 'NN'), ('district', 'NN'), ('attorney', 'NN'), ('announced', 'VBD'), ('monday', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "#POS Tagging\n",
    "## Full form of abbrevations : https://www.guru99.com/pos-tagging-chunking-nltk.html#:~:text=POS%20Tagging%20(Parts%20of%20Speech,of%20Speech)%20to%20each%20word.\n",
    "pos_tagged=nltk.pos_tag(wnl_tokens)\n",
    "print(pos_tagged[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "270e4ddd-7122-4835-b54e-046f8fca7162",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_pos_tagged=nltk.ne_chunk(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "73d3ab27-366b-4add-b464-9265a60bb89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON mr./JJ) PERSON\n",
      "other ('mr.', 'JJ')\n",
      "(PERSON mr./NN brandt/NNP) PERSON\n",
      "other ('mr.', 'NN')\n",
      "other ('brandt', 'NNP')\n",
      "(PERSON mr./JJ) PERSON\n",
      "other ('mr.', 'JJ')\n",
      "(GPE u.s./JJ) GPE\n",
      "other ('u.s.', 'JJ')\n",
      "(GPE u.s./JJ) GPE\n",
      "other ('u.s.', 'JJ')\n",
      "(GPE u.s./JJ) GPE\n",
      "other ('u.s.', 'JJ')\n"
     ]
    }
   ],
   "source": [
    "for tree in chunked_pos_tagged:\n",
    "    if hasattr(tree,'label'):\n",
    "        print(tree,tree.label())\n",
    "        if tree.label=='NN':\n",
    "            print(' '.join([ child[0] for child in tree] ))\n",
    "        else:\n",
    "            for c in tree:\n",
    "                print('other',c)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "43474300-f9a3-424b-a0c9-d745047e207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=sent_tokenize(data)\n",
    "tokenized_sentences=[word_tokenize(sentence) for sentence in sentences]\n",
    "tagged_sentences=[nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "chunked_sentences=nltk.ne_chunk_sents(tagged_sentences,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "225825e6-c168-4a47-b6af-ab0eed0cffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_names(tree):\n",
    "    entity_names=[]\n",
    "    \n",
    "    if hasattr(tree,'label') and tree.label():\n",
    "        \n",
    "        if tree.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in tree]) )\n",
    "        else:\n",
    "            for child in tree:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "                \n",
    "    return entity_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e342cbc9-d135-46cd-9694-ec69187d4e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_names=[]\n",
    "for tree in chunked_sentences:\n",
    "    entity_names.extend(extract_entity_names(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "abc951c6-ebb1-45f0-9cf6-ce0d2077e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mr.', 'u.s.', 'u.s.', 'u.s.']\n"
     ]
    }
   ],
   "source": [
    "print(entity_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f58f4850-12f3-4c8d-87eb-c512c288e826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON : mr. zimmerman\n",
      "GPE : u.s.\n",
      "GPE : u.s.\n",
      "GPE : u.s.\n"
     ]
    }
   ],
   "source": [
    "for t in nltk.ne_chunk(nltk.pos_tag(word_tokenize(data))):\n",
    "    if hasattr(t,'label') and  t.label() !='S':\n",
    "        print(t.label(),':', ' '.join([ child[0] for child in t]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707050b-bccb-402b-a706-de25e89aff37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf3da4-db0b-4e86-8678-87e0e2a4df01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
